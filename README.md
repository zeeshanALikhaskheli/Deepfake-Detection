ğŸ§  DeepFake Detector
Explainable Detection of AI-Generated Face Manipulations Using CNN and Grad-CAM

ğŸ“… Dated: 12-02-2026
ğŸ« Quaid-e-Awam University of Engineering, Science & Technology

ğŸ‘¨â€ğŸ’» Authors

Zeeshan Ali

Muhammad Mudasir

Amanat Ali

Muhammad Hammad Khan

Ms. Aisha Chandio

ğŸ“Œ Introduction

Artificial Intelligence has advanced to a level where it can generate highly realistic human face images, known as DeepFakes, using deep learning techniques.

These fake images look so real that it becomes very difficult for the human eye to distinguish them from real images.

Deepfakes are increasingly misused in:

Fake news

Social media manipulation

Identity theft

Cybercrimes

Reputation damage

Manual detection is unreliable and time-consuming.
Therefore, there is a strong need for an automatic and intelligent deepfake detection system.

â— Problem Statement

Artificial Intelligence can generate fake human face images that look almost real.

Although some AI tools can detect deepfakes, most systems only provide a â€œRealâ€ or â€œFakeâ€ result without explaining the reason.

This reduces:

User trust

Transparency

Interpretability

ğŸ‘‰ Therefore, we need a system that:

Detects fake images accurately

Highlights manipulated regions

Provides understandable visual explanations

ğŸ¯ Aims & Objectives

The main aim of this project is:

To develop a CNN-based deepfake detection system that accurately identifies AI-generated facial images and highlights manipulated regions using visual explanations.

Objectives:

Develop a CNN-based model for deepfake detection.

Integrate Grad-CAM to visually explain model decisions.

Build an easy-to-use web application for users.

ğŸ—ï¸ Proposed Methodology
1ï¸âƒ£ Data Collection & Preparation

Collected real and AI-generated images from public datasets.

Included different types of deepfake manipulations.

Preprocessing Steps:

Resize images to 224 Ã— 224

Normalize pixel values

Label images as Real / Fake

Apply data augmentation to reduce overfitting

2ï¸âƒ£ Model Architecture & Training

We used:

ğŸ”¹ Transfer Learning with EfficientNetB2

Pre-trained EfficientNetB2 as feature extractor

CNN-based architecture

Training Parameters:

Optimizer: Adam

Learning Rate: 0.0005

Loss Function: Binary Cross-Entropy

Batch Size: 32

Early Stopping to prevent overfitting

This improved generalization and training stability.

3ï¸âƒ£ Grad-CAM Integration (Explainable AI)

To improve transparency, we integrated Grad-CAM (Gradient-weighted Class Activation Mapping).

How it works:

Compute gradients of predicted class

Calculate neuron importance weights

Generate localization map

Overlay heatmap on original image

Heatmap Meaning:

ğŸ”´ Red/Yellow â†’ High manipulation relevance

ğŸ”µ Blue â†’ Low importance

This makes the system explainable and trustworthy.

4ï¸âƒ£ Web Application Development
ğŸ”¹ Backend:

Python Flask

Handles:

Image upload

Model inference

Grad-CAM generation

ğŸ”¹ Frontend:

React.js

User-friendly interface

Simple image upload and result display

ğŸŒ Web Application Workflow

1ï¸âƒ£ User uploads an image
2ï¸âƒ£ User clicks â€œAnalyze Imageâ€
3ï¸âƒ£ Backend processes the image
4ï¸âƒ£ Model predicts Real/Fake
5ï¸âƒ£ Grad-CAM heatmap is generated
6ï¸âƒ£ Final output shows:

Prediction

Visual heatmap

Transparent explanation

ğŸ“Š Results
1ï¸âƒ£ Model Performance Evaluation

(Add your performance metrics here)

Example format:

Accuracy: XX%

Precision: XX%

Recall: XX%

F1-Score: XX%

![Accuracy Graph](images/accuracy.png)
![Loss Graph](images/loss.png)
![Confusion Matrix](images/confusion_matrix.png)

2ï¸âƒ£ Grad-CAM Explainability Results

The system successfully highlights:

Eyes

Mouth

Skin blending areas

Facial boundaries

![GradCAM Result](images/gradcam_output.png)

ğŸ“Œ Result Summary

âœ… Accurate Deepfake Detection
Correctly identifies real and fake images with high reliability.

âœ… Explainable Visual Results
Grad-CAM heatmaps clearly show manipulated areas.

âœ… Easy-to-Use Web Application
Simple upload and instant result.

âœ… Reduced Misinformation
Helps prevent the spread of fake content.

âœ… Support for Trustworthy AI
Improves transparency in AI-based systems.

âš ï¸ Limitations

Image-based analysis only

Sensitive to dataset variations

ğŸš€ Future Work

Video deepfake detection

Hybrid deep learning models

Cross-dataset evaluation

Real-time deployment

ğŸ“š References

Arshed et al., Multiclass AI-generated deepfake face detection, 2024

Irfan et al., Deepfake generation and detection survey, 2025

Kapoor et al., CNN-based deepfake detection, 2025

Kolagati et al., CNN-MLP model, 2022

Mansoor & Iliev, Explainable AI for deepfake detection, 2025

Devi & Simon, DeepGuardNet, 2025

YardÄ±mcÄ± et al., ResNeXtâ€“LSTM video detection, 2025

ğŸ Conclusion

This project presents an explainable deepfake detection system using CNN and Grad-CAM.

Unlike traditional black-box models, our system:

Detects fake images

Explains the reason

Highlights manipulated regions

Improves user trust

This work contributes toward building more transparent and trustworthy AI systems.
